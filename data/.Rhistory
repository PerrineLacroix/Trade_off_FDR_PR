chi_p[[i]] <- lapply(gauss_variable, function(x) sum(x^p))
hist(unlist(chi_p[[i]]),breaks = seq(10,1000,by=10),main = paste("sum_1^n_Xsi^p_with","n=",n,"p=",round(p,2),"nbr_it",nbr_it,sep="_"),xlab="sum_chi_p")
print(p)
}
rm(list=objects())
n=100
nbr_it = 1000
gauss_variable <- lapply(1:nbr_it, function(it) abs(rnorm(n,mean = 0,sd = 1)))
p_vect=seq(1,4,length.out = 10)
chi_p <- list()
for (i in 1:length(p_vect)){
p = p_vect[i]
chi_p[[i]] <- lapply(gauss_variable, function(x) sum(x^p))
hist(unlist(chi_p[[i]]),breaks = seq(10,1000,by=10),main = paste("sum_1^n_|Xsi|^p_with","n=",n,"p=",round(p,2),"nbr_it",nbr_it,sep="_"),xlab="sum_chi_p")
print(p)
}
for (i in 1:length(p_vect)){
p = p_vect[i]
chi_p[[i]] <- lapply(gauss_variable, function(x) sum(x^p))
hist(unlist(chi_p[[i]]),breaks = seq(10,1000,by=10),ylim = c(0,500),main = paste("sum_1^n_|Xsi|^p_with","n=",n,"p=",round(p,2),"nbr_it",nbr_it,sep="_"),xlab="sum_chi_p")
print(p)
}
rm(list=objects())
n=100
nbr_it = 1000
gauss_variable <- lapply(1:nbr_it, function(it) abs(rnorm(n,mean = 0,sd = 1)))
gauss_variable
p_vect=seq(1,4,length.out = 10)   # les differents p
p_vect
gauss_variable <- lapply(1:nbr_it, function(it) abs(rnorm(n,mean = 0,sd = 1)))   # les gaussiennes
chi_p <- list()
gauss_variable[[1]]
rm(list=objects())
n=100   # le n de la somme
nbr_it = 1000    # Monte Carlo
p_vect=seq(1,4,length.out = 10)   # les differents p
gauss_variable <- lapply(1:nbr_it, function(it) abs(rnorm(n,mean = 0,sd = 1)))   # abs des gaussiennes
chi_p <- list()
for (i in 1:length(p_vect)){
p = p_vect[i]
chi_p[[i]] <- lapply(gauss_variable, function(x) sum(x^p))    # donne un vecteur de taille 1000
# chaque element est la realisation d'un chi_p
hist(unlist(chi_p[[i]]),breaks = seq(10,1000,by=10),ylim = c(0,500),main = paste("sum_1^n_|Xsi|^p_with","n=",n,"p=",round(p,2),"nbr_it",nbr_it,sep="_"),xlab="sum_chi_p")
}
rm(list=objects())
n=100   # le n de la somme
nbr_it = 1000    # Monte Carlo
p_vect=seq(1,4,length.out = 10)   # les differents p
pdf("chi_p_distribution.pdf")
gauss_variable <- lapply(1:nbr_it, function(it) abs(rnorm(n,mean = 0,sd = 1)))   # abs des gaussiennes
chi_p <- list()
for (i in 1:length(p_vect)){
p = p_vect[i]
chi_p[[i]] <- lapply(gauss_variable, function(x) sum(x^p))    # donne un vecteur de taille 1000
# chaque element est la realisation d'un chi_p
hist(unlist(chi_p[[i]]),breaks = seq(10,1000,by=10),ylim = c(0,500),main = paste("sum_1^n_|Xsi|^p_with","n=",n,"p=",round(p,2),"nbr_it",nbr_it,sep="_"),xlab="sum_chi_p")
}
dev.off()
x= c(1,2,3)
y= c(1,1,1)
x
y
z = c(x,y)
z
p = 271
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
271/135
p = 270
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
(p-1)/2
p = 270
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
p = 270
sum(sapply(0:(p-1), function(k) (cos((2*pi*k)/(p-1)))^2))
p = 270
(p+1)/2
(p-1)/2
sum(sapply(0:(p-1), function(k) (cos((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) cos((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((4*pi*k)/(p-1))))
p = 271
(p+1)/2
(p-1)/2
sum(sapply(0:(p-1), function(k) (cos((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) cos((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((4*pi*k)/(p-1))))
p = 308
(p+1)/2
p = 802
(p+1)/2
(p-1)/2
sum(sapply(0:(p-1), function(k) (cos((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) cos((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((4*pi*k)/(p-1))))
p = 302
(p+1)/2
(p-1)/2
sum(sapply(0:(p-1), function(k) (cos((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) (sin((2*pi*k)/(p-1)))^2))
sum(sapply(0:(p-1), function(k) cos((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((2*pi*k)/(p-1))))
sum(sapply(0:(p-1), function(k) sin((4*pi*k)/(p-1))))
K = seq(1,10,by=1)
plot(K,(K(K+1)^2)/(K-1)^3)
K = seq(1,10,by=1)
plot(K,((K(K+1)^2)/((K-1)^3)))
plot(K,((K*(K+1)^2)/((K-1)^3)))
sessionInfo
sessionInfo()
hist(rnorm(1000000,0,1))
hist(rnorm(10000000,0,1))
hist(rnorm(100000000,0,1))
moy <- 0 # moyenne de la population.
std <- 1 # écart-type de la population.
plot(function(x) dnorm(x,moy,std),  moy-4*std, moy+4*std,main = "",lwd=3,xlab="Taille des pièces (en mm)",ylab="Fréquence")
abline (v =c(-3,3), col="red",lwd=2,lty=3)
plot(function(x) dt(x,1),  ,main = "",lwd=3,xlab="Taille des pièces (en mm)",ylab="Fréquence")
plot(function(x) dt(x,1),  moy-4*std, moy+4*std,main = "",lwd=3,xlab="Taille des pièces (en mm)",ylab="Fréquence")
plot(function(x) dnorm(x,moy,std),  -10, 10,main = "",lwd=3,xlab="Taille des pièces (en mm)",ylab="Fréquence")
abline (v =c(-3,3), col="red",lwd=2,lty=3)
plot(function(x) dt(x,1),  -10, 10,main = "",lwd=3,xlab="Taille des pièces (en mm)",ylab="Fréquence")
install.packages("ElemStatLearn") install.packages("glmnet")
install.packages("ElemStatLearn")
install.packages("glmnet")
install.packages("ElemStatLearn")
install.packages("glmnet")
library(glmnet)
#install.packages("ElemStatLearn")
#install.packages("glmnet")
library(glmnet)
data("prostate", package = "ElemStatLearn") Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c("lpsa","train")]) ridge.out = glmnet(x=X,y=Y,alpha=0)
#install.packages("ElemStatLearn")
#install.packages("glmnet")
library(ElemStatLearn)
install.packages("ElemStatLearn")
install.packages("ElemStatLearn")
install.packages("/home/lacroix/Téléchargements/ElemStatLearn_2015.6.26.2.tar.gz")
install.packages("/home/lacroix/Téléchargements/ElemStatLearn_2015.6.26.2")
install.packages("/home/lacroix/Téléchargements/ElemStatLearn_2015.6.26.2/ElemStatLearn")
install.packages("/home/lacroix/Téléchargements/ElemStatLearn_0.1-7")
#install.packages("ElemStatLearn")
#install.packages("glmnet")
library(ElemStatLearn)
library(glmnet)
data("prostate", package = "ElemStatLearn") Y = prostate$lpsa
data("prostate", package = "ElemStatLearn")
Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c("lpsa","train")]) ridge.out = glmnet(x=X,y=Y,alpha=0)
X = as.matrix(prostate[,names(prostate)!=c("lpsa","train")])
ridge.out = glmnet(x=X,y=Y,alpha=0)
plot(ridge.out)
lasso.out = glmnet(x=X,y=Y,alpha=1)
plot(lasso.out)
head(X)
help("glmnet")
ridge.out
pridge.out = predict(ridge.out, x=X, s = 0.001, type = "response")
pridge.out = predict(ridge.out, x=X, s = 0.001, type = "response")
plot(ridge.out)
plot(ridge.out,label = TRUE)
plot(ridge.out,label = TRUE,x = X)
ridge.out$a0
ridge.out = glmnet(x=X,y=Y,family = "gaussian",alpha=0)
plot(ridge.out,label = TRUE)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.glmnet(x=X,y=Y,family = "gaussian",alpha=0)
help("cv.glmnet")
cv.glmnet(x=X,y=Y,type.measure="mse",nfolds=10,alpha=0)
cv.ridge.out = cv.glmnet(x=X,y=Y,type.measure="mse",nfolds=10,alpha=0)
plot(cv.ridge.out)
plot(ridge.out, sign.lambda = 1)
plot(ridge.out,label = TRUE)
plot(ridge.out, sign.lambda = 1,se.bands = TRUE)
plot(cv.ridge.out)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.ridge.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=0)
plot(cv.ridge.out)
title("Gaussian Family", line = 2.5)
relax.cv.ridge.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=0,relax = TRUE)
plot(relax.cv.ridge.out)
plot(ridge.out,label = TRUE)
cv.ridge.out
ridge.out$lambda
data("prostate", package = "ElemStatLearn")
Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c("lpsa","train")])
ridge.out = glmnet(x=X,y=Y,family = "gaussian",alpha=0)
plot(ridge.out,label = TRUE)
cv.ridge.out$lambda
cv.ridge.out$cvm
cv.ridge.out$nzero
cv.ridge.out$glmnet.fit
cv.ridge.out$lambda.min
cv.ridge.out
ridge.out$beta
dim(ridge.out$beta)
dim(X)
sapply(1:8,function(i) which(ridge.out$beta[i,]!=0)
)
length(Y)
dim(X)
data("prostate", package = "ElemStatLearn")
Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c("lpsa","train")])
ridge.out = glmnet(x=X,y=Y,family = "gaussian",alpha=0)
plot(ridge.out,label = TRUE)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.ridge.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=0)
plot(cv.ridge.out)
title("Gaussian Family", line = 2.5)
cv.ridge.out
help("cv.glmnet")
cv.ridge.out$lambda
cv.ridge.out$cvm
cv.ridge.out$nzero
cv.ridge.out$glmnet.fit
cv.ridge.out$lambda.min
cv.ridge.out$lambda.1se
cv.ridge.out$lambda
which(cv.ridge.out$lambda = cv.ridge.out$lambda.min)
which(cv.ridge.out$lambda == cv.ridge.out$lambda.min)
cv.ridge.out
which(cv.ridge.out$lambda == cv.ridge.out$lambda.1se)
abline(v=cv.ridge.out$lambda.1se)
title("Gaussian Family", line = 2.5)
lasso.out = glmnet(x=X,y=Y,alpha=1)
plot(lasso.out)
plot(lasso.out,label = TRUE)
lasso.out = glmnet(x=X,y=Y,alpha=1)
plot(lasso.out,label = TRUE)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.lasso.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=1)
plot(cv.lasso.out)
plot(ridge.out,label = TRUE)
abline(v=cv.ridge.out$lambda.1se)
plot(cv.lasso.out)
abline(v=cv.lasso.out$lambda.1se)
plot(lasso.out,label = TRUE)
abline(v=cv.lasso.out$lambda.1se)
cv.lasso.out$lambda.1se
cv.lasso.out$lambda
abline(v=cv.lasso.out$lambda.1se,col=3)
abline(v=cv.lasso.out$lambda.1se,col=3,line=2)
abline(v=cv.lasso.out$lambda.1se,col=3,lwd=2)
abline(v=cv.lasso.out$lambda.1se,col=3,lwd=3)
title("Lasso penalty", line = 2.5)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.lasso.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=1)
plot(cv.lasso.out)
title("Gaussian Family", line = 2.5)
library(ElemStatLearn)
library(glmnet)
data("prostate", package = "ElemStatLearn")
Y = prostate$lpsa
X = as.matrix(prostate[,names(prostate)!=c("lpsa","train")])
ridge.out = glmnet(x=X,y=Y,family = "gaussian",alpha=0)
plot(ridge.out,label = TRUE)
abline(v=cv.ridge.out$lambda.1se)
title("Rigde penalty", line = 2.5)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.ridge.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=0)
plot(cv.ridge.out)
title("Gaussian Family", line = 2.5)
lasso.out = glmnet(x=X,y=Y,alpha=1)
plot(lasso.out,label = TRUE)
abline(v=cv.lasso.out$lambda.1se,col=3,lwd=3)
title("Lasso penalty", line = 2.5)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.lasso.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=1)
plot(cv.lasso.out)
title("Gaussian Family", line = 2.5)
cv.lasso.out
lasso.out
cv.lasso.out$lambda.1se
abline(v=log(cv.lasso.out$lambda.1se),col=3,lwd=3)
## LASSO
lasso.out = glmnet(x=X,y=Y,alpha=1)
plot(lasso.out,label = TRUE)
abline(v=log(cv.lasso.out$lambda.1se),col=3,lwd=3)
title("Lasso penalty", line = 2.5)
lasso.out$nobs
lasso.out$offset
lasso.out$jerr
lasso.out$dev.ratio
lasso.out$a0
lasso.out$df
lasso.out$lambda
lasso.out$dev.ratio
lasso.out$nulldev
lasso.out$npasses
cv.lasso.out$lambda
cv.lasso.out$lambda.1se
which(cv.lasso.out$lambda.1se==cv.lasso.out$lambda)
dim(lasso.out$beta)
lasso.out$beta[,which(cv.lasso.out$lambda.1se==cv.lasso.out$lambda)]
length(Y)
dim(X)
sum(abs(lasso.out$beta[,which(cv.lasso.out$lambda.1se==cv.lasso.out$lambda)]))
plot(lasso.out,label = TRUE)
abline(v=sum(abs(lasso.out$beta[,which(cv.lasso.out$lambda.1se==cv.lasso.out$lambda)])),col=3,lwd=3)
## RIDGE
ridge.out = glmnet(x=X,y=Y,family = "gaussian",alpha=0)
plot(ridge.out,label = TRUE)
abline(v=sum(abs(ridge.out$beta[,which(cv.ridge.out$lambda.1se==cv.ridge.out$lambda)])),col=3,lwd=3)
title("Rigde penalty", line = 2.5)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.ridge.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=0)
plot(cv.ridge.out)
title("Gaussian Family", line = 2.5)
## LASSO
lasso.out = glmnet(x=X,y=Y,alpha=1)
plot(lasso.out,label = TRUE)
abline(v=sum(abs(lasso.out$beta[,which(cv.lasso.out$lambda.1se==cv.lasso.out$lambda)])),col=3,lwd=3)
title("Lasso penalty", line = 2.5)
# J'ai rajoute avec "label=TRUE" le numero des variables sélectionnees
cv.lasso.out = cv.glmnet(x=X,y=Y,nfolds=10,alpha=1)
plot(cv.lasso.out)
title("Gaussian Family", line = 2.5)
x = 1:10
x
plot(x,exp(-x))
lines(x,exp(-10*x), col=2)
lines(x,exp(-0.1*x), col=2)
x = 1:10
plot(x,exp(-x), ylim = c(0,max(exp(-0.1*x))))
lines(x,exp(-10*x), col=2)
lines(x,exp(-0.1*x), col=2)
lines(x,exp(-0.8*x), col=2)
lines(x,exp(-0.5*x), col=2)
lines(x,exp(-5*x), col=2)
lines(x,exp(-0.1*x), col=2)
lines(x,exp(-0.5*x), col=2)
lines(x,exp(-0.8*x), col=2)
lines(x,exp(-0.9*x), col=2)
lines(x,exp(-0.2*x), col=2)
lines(x,exp(-0.3*x), col=2)
lines(x,exp(-0.4*x), col=2)
lines(x,exp(-0.5*x), col=2)
lines(x,exp(-5*x), col=2)
lines(x,exp(-0.5*x), col=2)
x = 1:10
plot(x,exp(-x), ylim = c(0,max(exp(-0.1*x))))
lines(x,exp(-5*x), col=2)
lines(x,exp(-0.5*x), col=2)
exp(-x)
exp(-5*x)
exp(-0.5*x)
x = 1:10
plot(x,exp(-x), ylim = c(0,max(exp(-0.1*x))))
lines(x,exp(-5*x), col=2)
1/sqrt(50)
exp(-0.5*x) > 1/sqrt(50)
exp(-5*x) > 1/sqrt(50)
exp(-x) > 1/sqrt(50)
exp(-(x/50)) > 1/sqrt(50)
exp(-(x/50))
rm(list=objects())
prefix = "~/Documents/GitHub/Trade_off_FDR_PR"
setwd(paste(prefix,"/data",sep =""))
# Parameters of simulation
nbr_it = 1000   # Data sets number for the empirical estimations of FDR and PR.
nbr_it_bound = 100   # Data sets number to compute the FDR bounds and to study the variability of the FDR bounds.
n = 50  # number of observations
p = 50   # number of variables
n_temps = 2*n*nbr_it   # Required total iterations number to get nbr_it train sets for the empirical estimation of FDR, with nbr_it validation sets for the empirical estimation of PR.
n_temps_bounds = n*nbr_it_bound   # Required total iterations number to get nbr_it_bound data sets for the FDR bounds and the variability studies of the FDR bounds.
sigma_2 <- 1   # Variance of the noise in the Gaussian linear regression
## parameters of the created data for the saving
config_data = paste0(paste("sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
###### Creation of the matrix X and noise: X is fixed for all iterations since the design is supposed to be fixed in the model.
## Here, the design matrix is fixed and we choose the usual orthonormal matrix for each iteration.
## Only the noise is random.
set.seed(1234)
paste("matrix_X",config_data,sep="_")
paste("matrix_X",config_data,sep="_")
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,
"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
paste("vector_Y",config_data,sep="_")
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,
"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
paste("smaller_noise_vector_Y",config_data,sep="_")
paste("larger_noise_closed_values_vector_Y",config_data,sep="_")
config_data = paste0(paste("sigma2",sigma_2,
"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
paste("vector_Y_noise_bounds",config_data,sep="_")
paste("vector_Y_smaller_noise_noise_bounds",config_data,sep="_")
paste("vector_Y_larger_noise_closed_values_bounds",config_data,sep="_")
config_data = paste0(paste("sigma2",sigma_2,
"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
paste("path_collection",config_data,sep="_")
paste("smaller_noise_path_collection",config_data,sep="_")
paste("larger_noise_closed_values_path_collection",config_data,sep="_")
paste("path_collection_bounds",config_data,sep="_")
paste("path_collection_bounds_smaller_noise",config_data,sep="_")
paste("path_collection_bounds_larger_noise_closed_values",config_data,sep="_")
rm(list=objects())
prefix = "~/Documents/GitHub/Trade_off_FDR_PR"
setwd(paste(prefix,"/data",sep =""))
# Parameters of simulation
nbr_it = 1000   # Data sets number for the empirical estimations of FDR and PR.
nbr_it_bound = 100   # Data sets number to compute the FDR bounds and to study the variability of the FDR bounds.
n = 50  # number of observations
p = 50   # number of variables
n_temps = 2*n*nbr_it   # Required total iterations number to get nbr_it train sets for the empirical estimation of FDR, with nbr_it validation sets for the empirical estimation of PR.
n_temps_bounds = n*nbr_it_bound   # Required total iterations number to get nbr_it_bound data sets for the FDR bounds and the variability studies of the FDR bounds.
sigma_2 <- 1   # Variance of the noise in the Gaussian linear regression
## parameters of the created data for the saving
config_data = paste0(paste("sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data = paste0(paste("sigma2",sigma_2,
"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data
paste("path_collection_deterministic",config_data,sep="_")
paste("path_collection_smaller_noise_deterministic",config_data,sep="_")
paste("path_collection_larger_noise_closed_values_deterministic",config_data,sep="_")
paste("path_collection_bounds_deterministic",config_data,sep="_")
paste("path_collection_bounds_smaller_noise_deterministic",config_data,sep="_")
paste("path_collection_bounds_larger_noise_closed_values_deterministic",config_data,sep="_")
paste("path_collection_slope_random",config_data,sep="_")
paste("path_collection_bolasso_random",config_data,sep="_")
paste("path_collection_bounds_bolasso_random",config_data,sep="_")
paste("path_collection_random",config_data,sep="_")
file=paste("path_collection_bounds_random",config_data,sep="_")
file=paste("path_collection_bounds_random",config_data,sep="_")
paste("path_collection_bounds_random",config_data,sep="_")
paste("path_collection_larger_noise_closed_values_random",config_data,sep="_")
paste("path_collection_bounds_larger_noise_closed_values_random",config_data,sep="_")
paste("path_collection_smaller_noise_random",config_data,sep="_")
paste("path_collection_bounds_smaller_noise_random",config_data,sep="_")
# Loading the generated data sets
config_data = paste0(paste("sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
## script to launch simulations
## author : perrine lacroix
## date : March, 10 2021
## This code is the main R script.
## From the created data sets and the generated model collections (from Git_multiplicative_constant_K_creation_data_and_collection.R and Git_multiplicative_constant_K_collection_non_ordered.R R functions),
## the PR and the FDR are firstly computed for each K and for each iteration.
## Then, the P_{2r} term in the FDR expression is estimated by an empirical process.
## Lower bounds and upper bounds are computed, for each K and for all the \Tilde{K} considered in the article.
## The rest of the R script stands for plottings and for comparaison between our algorithm and some existing variable selection procedures.
rm(list=objects())
prefix = "~/Documents/GitHub/Trade_off_FDR_PR"
# Packages loading
library(xtable)
library(ggplot2)
library(capushe)
library(LINselect)
library(knockoff)
library(caret)
## Loading of coded function
setwd(paste(prefix,"/source",sep =""))
source("Git_cste_mult_variation_PR_FDP.R")
source("Git_empirical_estimation.R")
source("Git_P_2r_estimation.R")
source("Git_lower_bounds.R")
source("Git_upper_bounds.R")
source("Git_lower_bound_estimated_one_dataset.R")
source("Git_upper_bound_estimated_one_dataset.R")
source("Git_empirical_estimation_collection_non_ordered.R")
source("Git_algorithm_trade_off.R")
source("Git_LinSelect.R")
## Loading of the dataset
setwd(paste(prefix,"/data",sep =""))
# Parameters of simulation
nbr_it = 1000   # Data sets number for the empirical estimations of FDR and PR.
nbr_it_bound = 100   # Data sets number to compute the FDR bounds and to study the variability of the FDR bounds.
n = 50  # number of observations
p = 50   # number of variables
sigma_2 <- 1  # Variance of the noise in the Gaussian linear regression
cste_mult_vect <- seq(0.1,10,length.out = 20)    # The considered K constants
cste_mult_vect <- c(cste_mult_vect[which(cste_mult_vect<2)],2,cste_mult_vect[which(cste_mult_vect>2)])    # To add the constant 2 in the studied K
asympt_number <- 5000    # Number for the empirical estimation of the P_{2r} term in the FDR expression.
alpha_FDR <- 0.05    # threshold for the knockoff method
ordered_variable <- TRUE    # TRUE for the deterministic ordered model collections; FALSE for the deterministic non ordered model collections and for random collections
random_collection <- FALSE    # TRUE for random collections; FALSE for the deterministic ordered model collections and for the deterministic non ordered model collections
# Loading the generated data sets
config_data = paste0(paste("sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data
config_data
# Loading the generated data sets
config_data = paste0(paste("sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data = paste0(paste("beta_true_0_20","sigma2",sigma_2,"p",p,"n",n,"nbr_it",nbr_it,sep="_"),".RData")
config_data
config_data
paste("PR_FDP_hat_K",config_data,sep="_")
paste("smaller_noise_PR_FDP_hat_K",config_data,sep="_")
paste0(paste("estimation_of_P_2r_for_all_D_m_in_0_q","p",p,"n",n,"asympt_number",asympt_number,sep="_"),".RData")
paste("lower_bounds_all",config_data,sep="_")
paste("smaller_noise_lower_bounds_all",config_data,sep="_")
paste("larger_noise_closed_values_lower_bounds_all",config_data,sep="_")
